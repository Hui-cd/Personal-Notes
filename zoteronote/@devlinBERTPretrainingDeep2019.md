# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

## Metadata
- **CiteKey**: {{citekey}}
 - **Type**: Preprint
 - **Title**: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 
 - **Author**: Devlin, Jacob; Chang, Ming-Wei; Lee, Kenton; Toutanova, Kristina, 
 - **Year**: 2019 ;
- **Journal**: {{publicationTitle}}, 
- **Pages**: {{pages}}
- **Publisher**: {{publisher}},
- **Location**: {{place}},
- **DOI**: {{DOI}}
------


## Files and Links
- **Url**: [Open online](http://arxiv.org/abs/1810.04805)
- **zotero entry**: [Zotero](zotero://select/library/items/YEFMDXK5)
- **open pdf**: [Devlin 等 - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf](zotero://open-pdf/library/items/K4FEEV4X)

- **Keywords**: Computer Science - Computation and Language

## Abstract
We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.

----

## Comments



----

## Extracted Annotations

****



## Summary

  
## Research Objective(s)


## Background / Problem Statement


## Method(s)


## Evaluation


## Conclusion


## Notes
